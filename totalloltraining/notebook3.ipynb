{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db87dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e763c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5d63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../TotalLoL\"\n",
    "\n",
    "TRAIN_LOW = os.path.join(BASE_PATH, \"train\", \"low\")\n",
    "TRAIN_HIGH = os.path.join(BASE_PATH, \"train\", \"high\")\n",
    "\n",
    "RESULTS_PATH = \"../results\"\n",
    "ILLUM_MODEL_PATH = os.path.join(RESULTS_PATH, \"illumination_net_best_totalLol_training.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4f4050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LOLDataset(Dataset):\n",
    "    def __init__(self, low_dir, high_dir):\n",
    "        self.low_images = sorted(os.listdir(low_dir))\n",
    "        self.high_images = sorted(os.listdir(high_dir))\n",
    "        self.low_dir = low_dir\n",
    "        self.high_dir = high_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.low_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low = cv2.imread(os.path.join(self.low_dir, self.low_images[idx]))\n",
    "        high = cv2.imread(os.path.join(self.high_dir, self.high_images[idx]))\n",
    "\n",
    "        low = cv2.cvtColor(low, cv2.COLOR_BGR2RGB)\n",
    "        high = cv2.cvtColor(high, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        low = torch.from_numpy(low / 255.0).float().permute(2, 0, 1)\n",
    "        high = torch.from_numpy(high / 255.0).float().permute(2, 0, 1)\n",
    "\n",
    "        return low, high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e23a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LOLDataset(TRAIN_LOW, TRAIN_HIGH)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44f0a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IlluminationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(32, 1, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fdabe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illumination model loaded and frozen.\n"
     ]
    }
   ],
   "source": [
    "illum_model = IlluminationNet().to(device)\n",
    "illum_model.load_state_dict(torch.load(ILLUM_MODEL_PATH, map_location=device))\n",
    "illum_model.eval()\n",
    "\n",
    "for param in illum_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Illumination model loaded and frozen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a61412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_illumination(low_img, illum, eps=1e-4):\n",
    "    return low_img / (illum + eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5410081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically it is a reusable building block it is being used in the multilscale unet multiple times\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),  # extract features \n",
    "            nn.ReLU(), # and it adds non linearity to the model\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1), # extract features \n",
    "            nn.ReLU() # and it adds non linearity to the model\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f872b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is full Enhancement Network with encoder and decoder and skip connections.....................\n",
    "class MultiScaleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc1 = ConvBlock(3, 64)  \n",
    "        self.enc2 = ConvBlock(64, 128) \n",
    "        self.pool = nn.MaxPool2d(2) # this is max pooling layer it will reduce the spatial dimensions by half so the output will be 128 * 128 * 128 (this will move to bottleneck)\n",
    "\n",
    "        self.bottleneck = ConvBlock(128, 256) # here the convblock will extract features and the output will be 64 * 64 * 256 (this is the bottleneck layer)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x) # suppose the image is 256 * 256 * 3 the output will be 256 * 256 * 64\n",
    "        e2 = self.enc2(self.pool(e1)) # the input will be 256 * 256 * 64 and the output will be 256 * 256 * 128  THIS IS ALSO CALLED DOWNSAMPLING\n",
    "\n",
    "\n",
    "        b = self.bottleneck(self.pool(e2)) # pooling e2 (128 × 128 × 128 #⬇️ pooling 64 × 64 × 128)\n",
    "        #Pooling reduced spatial size, and the convolution inside the bottleneck increased the number of channels from 128 to 256.\n",
    "        \n",
    "        # UPSAMPLING\n",
    "\n",
    "        d2 = self.up2(b) # 128 × 128 × 128\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1)) # Skip connection + channel explosion (on purpose)  #Decoder ConvBlock reduces channels \n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))  \n",
    "\n",
    "        return torch.sigmoid(self.final(d1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f25ca0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "def edge_loss(pred, target):\n",
    "    pred_gray = torch.mean(pred, dim=1, keepdim=True)\n",
    "    target_gray = torch.mean(target, dim=1, keepdim=True)\n",
    "    return torch.mean(torch.abs(pred_gray - target_gray))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc7e23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_model = MultiScaleUNet().to(device)\n",
    "optimizer = optim.Adam(refine_model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57125e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running one-batch sanity check...\n",
      "One batch forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "# ---- SANITY CHECK: ONE BATCH ----\n",
    "print(\"Running one-batch sanity check...\")\n",
    "\n",
    "low_img, high_img = next(iter(loader))\n",
    "low_img = low_img.to(device)\n",
    "high_img = high_img.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    illum = illum_model(low_img)\n",
    "    illum_corrected = correct_illumination(low_img, illum)\n",
    "    out = refine_model(illum_corrected)\n",
    "\n",
    "print(\"One batch forward pass successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d230616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | Loss: 0.1586\n",
      "Epoch [2/50] | Loss: 0.1426\n",
      "Epoch [3/50] | Loss: 0.1413\n",
      "Epoch [4/50] | Loss: 0.1389\n",
      "Epoch [5/50] | Loss: 0.1365\n",
      "Epoch [6/50] | Loss: 0.1349\n",
      "Epoch [7/50] | Loss: 0.1324\n",
      "Epoch [8/50] | Loss: 0.1315\n",
      "Epoch [9/50] | Loss: 0.1303\n",
      "Epoch [10/50] | Loss: 0.1300\n",
      "Epoch [11/50] | Loss: 0.1285\n",
      "Epoch [12/50] | Loss: 0.1285\n",
      "Epoch [13/50] | Loss: 0.1269\n",
      "Epoch [14/50] | Loss: 0.1267\n",
      "Epoch [15/50] | Loss: 0.1259\n",
      "Epoch [16/50] | Loss: 0.1255\n",
      "Epoch [17/50] | Loss: 0.1250\n",
      "Epoch [18/50] | Loss: 0.1247\n",
      "Epoch [19/50] | Loss: 0.1249\n",
      "Epoch [20/50] | Loss: 0.1245\n",
      "Epoch [21/50] | Loss: 0.1235\n",
      "Epoch [22/50] | Loss: 0.1233\n",
      "Epoch [23/50] | Loss: 0.1224\n",
      "Epoch [24/50] | Loss: 0.1232\n",
      "Epoch [25/50] | Loss: 0.1221\n",
      "Epoch [26/50] | Loss: 0.1214\n",
      "Epoch [27/50] | Loss: 0.1215\n",
      "Epoch [28/50] | Loss: 0.1210\n",
      "Epoch [29/50] | Loss: 0.1207\n",
      "Epoch [30/50] | Loss: 0.1204\n",
      "Epoch [31/50] | Loss: 0.1195\n",
      "Epoch [32/50] | Loss: 0.1186\n",
      "Epoch [33/50] | Loss: 0.1186\n",
      "Epoch [34/50] | Loss: 0.1184\n",
      "Epoch [35/50] | Loss: 0.1182\n",
      "Epoch [36/50] | Loss: 0.1177\n",
      "Epoch [37/50] | Loss: 0.1171\n",
      "Epoch [38/50] | Loss: 0.1169\n",
      "Epoch [39/50] | Loss: 0.1159\n",
      "Epoch [40/50] | Loss: 0.1158\n",
      "Epoch [41/50] | Loss: 0.1159\n",
      "Epoch [42/50] | Loss: 0.1151\n",
      "Epoch [43/50] | Loss: 0.1151\n",
      "Epoch [44/50] | Loss: 0.1147\n",
      "Epoch [45/50] | Loss: 0.1139\n",
      "Epoch [46/50] | Loss: 0.1135\n",
      "Epoch [47/50] | Loss: 0.1128\n",
      "Epoch [48/50] | Loss: 0.1132\n",
      "Epoch [49/50] | Loss: 0.1118\n",
      "Epoch [50/50] | Loss: 0.1112\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    refine_model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for low_img, high_img in loader:\n",
    "        low_img = low_img.to(device)\n",
    "        high_img = high_img.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            illum = illum_model(low_img)\n",
    "            illum_corrected = correct_illumination(low_img, illum)\n",
    "\n",
    "        refined = refine_model(illum_corrected)\n",
    "\n",
    "        loss = l1_loss(refined, high_img) + 0.1 * edge_loss(refined, high_img)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2d705e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'refine_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrefine_model\u001b[49m.eval()\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      5\u001b[39m     low, high = dataset[\u001b[32m10\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'refine_model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "refine_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    low, high = dataset[10]\n",
    "    low = low.unsqueeze(0).to(device)\n",
    "\n",
    "    illum = illum_model(low)\n",
    "    illum_corrected = correct_illumination(low, illum)\n",
    "    refined = refine_model(illum_corrected)\n",
    "\n",
    "low = low.squeeze().permute(1,2,0).cpu().numpy()\n",
    "illum_corrected = illum_corrected.squeeze().permute(1,2,0).cpu().numpy()\n",
    "refined = refined.squeeze().permute(1,2,0).cpu().numpy()\n",
    "high = high.permute(1,2,0).numpy()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,4,1); plt.imshow(low); plt.title(\"Low\"); plt.axis(\"off\")\n",
    "plt.subplot(1,4,2); plt.imshow(illum_corrected); plt.title(\"Illum Corrected\"); plt.axis(\"off\")\n",
    "plt.subplot(1,4,3); plt.imshow(refined); plt.title(\"Refined\"); plt.axis(\"off\")\n",
    "plt.subplot(1,4,4); plt.imshow(high); plt.title(\"Ground truth\"); plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b815b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reflectance refinement model saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(refine_model.state_dict(), \"../results/reflectance_refine_net_50epoch_TotalLoL.pth\")\n",
    "print(\"Reflectance refinement model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
